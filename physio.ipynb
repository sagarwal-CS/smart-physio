{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import urllib.request\n",
    "import datetime\n",
    "import traceback\n",
    "import redis\n",
    "import face_recognition\n",
    "from time import sleep\n",
    "\n",
    "from config import load_config\n",
    "from nnet import predict\n",
    "from util import visualize\n",
    "from dataset.pose_dataset import data_to_input\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FOREHEAD = 'forehead'\n",
    "CHIN = 'chin'\n",
    "SHOULDER = 'shoulder'\n",
    "ELBOW = 'elbow'\n",
    "WRIST = 'wrist'\n",
    "HIP = 'hip'\n",
    "KNEE = 'knee'\n",
    "ANKLE = 'ankle'\n",
    "POSITION_1_PCT_X_THRESHOLD = 0.3\n",
    "POSITION_1_PCT_Y_THRESHOLD = 0.3\n",
    "JOINT_MATCH_THRESHOLD = 0.9\n",
    "CENTER_CHANGE_ALPHA = 0.8\n",
    "# body_graph = {}\n",
    "# body_graph[FOREHEAD] = [CHIN]\n",
    "# body_graph[CHIN] = [FOREHEAD, SHOULDER]\n",
    "# body_graph[SHOULDER] = [CHIN, ELBOW, HIP]\n",
    "# body_graph[ELBOW] = [SHOULDER, WRIST]\n",
    "# body_graph[WRIST] = [ELBOW]\n",
    "# body_graph[HIP] = [SHOULDER, KNEE]\n",
    "# body_graph[KNEE] = [HIP, ANKLE]\n",
    "# body_graph[ANKLE] = [KNEE]\n",
    "LEFT_RESTING_POSITION = {\n",
    "    'name': 'Left resting position',\n",
    "    'description': 'Left hand to the side of your hips resting loose.'\n",
    "}\n",
    "RIGHT_RESTING_POSITION = {\n",
    "    'name': 'Right resting position',\n",
    "    'description': 'Right hand to the side of your hips resting loose.'\n",
    "}\n",
    "LEFT_SIDE_POSITION = {\n",
    "    'name': 'Left side position'\n",
    "}\n",
    "RIGHT_SIDE_POSITION = {\n",
    "    'name': 'Right side position',\n",
    "}\n",
    "\n",
    "\n",
    "physiotherapy_routine1 = [\n",
    "    {'commentary': 'Today, we will do right hand movements. This physiotherapy routine will help you get back the full functioning of your right arms.',\n",
    "        'steps':[\n",
    "    LEFT_RESTING_POSITION,\n",
    "    LEFT_SIDE_POSITION,\n",
    "]*2},{\n",
    "        'commentary': 'You are doing good. Just 3 more!',\n",
    "        'steps': [\n",
    "            LEFT_RESTING_POSITION,\n",
    "            LEFT_SIDE_POSITION\n",
    "        ]*3\n",
    "    }\n",
    "]\n",
    "\n",
    "physiotherapy_routine2 = [\n",
    "    {'commentary': 'This routine will help you relieve your shoulder pain. Lets start with left hand movements.',\n",
    "        'steps':[\n",
    "    RIGHT_RESTING_POSITION,\n",
    "    RIGHT_SIDE_POSITION,\n",
    "]*2},{\n",
    "        'commentary': 'You are doing good. Now to the right 3 times.',\n",
    "        'steps': [\n",
    "            LEFT_RESTING_POSITION,\n",
    "            LEFT_SIDE_POSITION\n",
    "        ]*3\n",
    "    }\n",
    "]\n",
    "PROFILES = [\n",
    "{\n",
    "    'name': 'Person1',\n",
    "    'image_path': 'data/known_images/name_of_image.jpg',\n",
    "    'routine_spec': physiotherapy_routine1\n",
    "}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enrich_profile(profile):\n",
    "    profile['image'] = face_recognition.load_image_file(profile['image_path'])\n",
    "    profile['face_encoding'] = face_recognition.face_encodings(profile['image'])[0]\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/known_images/name_of_image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ed2a69286dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mENRICHED_PROFILES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprofile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPROFILES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mENRICHED_PROFILES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menrich_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-c869d178845b>\u001b[0m in \u001b[0;36menrich_profile\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0menrich_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'face_encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mload_image_file\u001b[0;34m(file, mode)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(name, flatten, mode)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \"\"\"\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2478\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/known_images/name_of_image.jpg'"
     ]
    }
   ],
   "source": [
    "ENRICHED_PROFILES = {}\n",
    "for profile in PROFILES:\n",
    "    ENRICHED_PROFILES[profile['name']] = enrich_profile(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/mpii/mpii-single-resnet-101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/mpii/mpii-single-resnet-101\n"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"pose_cfg.yaml\")\n",
    "# Load and setup CNN part detector\n",
    "sess, inputs, outputs = predict.setup_pose_prediction(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_stream(url):\n",
    "    stream = urllib.request.urlopen(url)\n",
    "    bytes1 = bytes()\n",
    "    while True:\n",
    "        bytes1 += stream.read(1024)\n",
    "        a = bytes1.find(b'\\xff\\xd8')\n",
    "        b = bytes1.find(b'\\xff\\xd9')\n",
    "        if a != -1 and b != -1:\n",
    "            jpg = bytes1[a:b+2]\n",
    "            bytes1 = bytes1[b+2:]\n",
    "            yield cv2.imdecode(np.fromstring(jpg, dtype=np.uint8), cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 5], [1, 4], [2, 3], [6, 11], [7, 10], [8, 9], [12], [13]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_joints = cfg.all_joints\n",
    "all_joints_names = cfg.all_joints_names\n",
    "all_joints_names\n",
    "all_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ankle', 'knee', 'hip', 'wrist', 'elbow', 'shoulder', 'chin', 'forehead']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_joints_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index(part, key, idx):\n",
    "    try:\n",
    "        return part[key][idx]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_confident_pose_parts(pose_dict):\n",
    "    filtered_pose_dict = {}\n",
    "    for joint_name, parts in pose_dict.items():\n",
    "        filtered_pose_dict[joint_name] = [p for p in parts if p[-1] > JOINT_MATCH_THRESHOLD]\n",
    "    return filtered_pose_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_pose(image):\n",
    "    image_batch = data_to_input(image)\n",
    "\n",
    "    # Compute prediction with the CNN\n",
    "    outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})\n",
    "    scmap, locref, _ = predict.extract_cnn_output(outputs_np, cfg)\n",
    "\n",
    "    # Extract maximum scoring location from the heatmap, assume 1 person\n",
    "    pose = predict.argmax_pose_predict(scmap, locref, cfg.stride)\n",
    "    return pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_exercise1_pos1(lr_parts):\n",
    "    dir_pos1_detected = {}\n",
    "    for direction, parts in lr_parts.items():\n",
    "        xs = np.array([get_index(parts, SHOULDER, 0), get_index(parts, WRIST, 0), get_index(parts, ELBOW, 0)])\n",
    "        mean = xs.mean()\n",
    "\n",
    "        # Within % of the mean\n",
    "        decision_on_x = all(abs((xs - mean) / xs) < POSITION_1_PCT_X_THRESHOLD)\n",
    "        \n",
    "        decision_on_y = get_index(parts, SHOULDER, 1) < get_index(parts, WRIST, 1) and get_index(parts, ELBOW, 1) < get_index(parts, WRIST, 1)\n",
    "        dir_pos1_detected[direction] = decision_on_x and decision_on_y\n",
    "    return dir_pos1_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_exercise1_pos2(lr_parts):\n",
    "    dir_pos_detected = {}\n",
    "    for direction, parts in lr_parts.items():\n",
    "        ys = np.array([get_index(parts, SHOULDER, 1), get_index(parts, WRIST, 1), get_index(parts, ELBOW, 1)])\n",
    "        mean = ys.mean()\n",
    "        # Within % of the mean\n",
    "        decision_on_y =  all(abs((ys - mean) / ys) < POSITION_1_PCT_Y_THRESHOLD)\n",
    "        if direction == 'left':\n",
    "            decision_on_x =  get_index(parts, SHOULDER, 0) > get_index(parts, WRIST, 0) and get_index(parts, ELBOW, 0) > get_index(parts, WRIST, 0)\n",
    "        else:\n",
    "            decision_on_x =  get_index(parts, SHOULDER, 0) < get_index(parts, WRIST, 0) and get_index(parts, ELBOW, 0) < get_index(parts, WRIST, 0)\n",
    "        dir_pos_detected[direction] = decision_on_x and decision_on_y\n",
    "    return dir_pos_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pose_to_dict(pose):\n",
    "    poses_dict = {}\n",
    "    for i, joint_name in enumerate(all_joints_names):\n",
    "        poses_dict[joint_name] = [pose[j] for j in all_joints[i]]\n",
    "    return poses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_left_right_parts(pose_dict, x_center):\n",
    "    left_parts = {}\n",
    "    right_parts = {}\n",
    "    for joint_name, jointcoords in pose_dict.items():\n",
    "        if joint_name != CHIN and joint_name != FOREHEAD:\n",
    "            for j in jointcoords:\n",
    "                if j[0] < x_center:\n",
    "                    left_parts[joint_name] = j\n",
    "                elif j[0] > x_center:\n",
    "                    right_parts[joint_name] = j\n",
    "    return {'left': left_parts, 'right': right_parts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_center_x(pose_dict):\n",
    "    \"\"\"\n",
    "        Finds the central point (x) to divide into left and right\n",
    "    \"\"\"\n",
    "    shoulders = np.array(pose_dict[SHOULDER])\n",
    "    matched_shoulders = shoulders[(shoulders[:,2] > JOINT_MATCH_THRESHOLD)]\n",
    "    hips = np.array(pose_dict[HIP])\n",
    "    matched_hips = hips[(hips[:,2] > JOINT_MATCH_THRESHOLD)]\n",
    "    chin = pose_dict[CHIN][0]\n",
    "    matched_chin = chin[chin[2] > JOINT_MATCH_THRESHOLD]\n",
    "    forehead = pose_dict[FOREHEAD][0]\n",
    "    matched_forehead = forehead[forehead[2] > JOINT_MATCH_THRESHOLD]\n",
    "\n",
    "\n",
    "    mean_shoulders = np.nan if(len(matched_shoulders) < 2) else np.mean(matched_shoulders, axis=0)\n",
    "    mean_shoulders\n",
    "\n",
    "    mean_hips = np.nan if(len(matched_hips) < 2) else np.mean(matched_hips, axis=0)\n",
    "    mean_hips\n",
    "\n",
    "    matched_shoulders, matched_hips, matched_chin, matched_forehead\n",
    "\n",
    "    mean_candidates = [mean_hips, mean_shoulders, matched_chin, matched_forehead]\n",
    "\n",
    "    return np.mean(np.array(list(filter(lambda m: not np.all(np.isnan(m)), mean_candidates))), axis=0)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_pose(frame, pose):\n",
    "    pose_dict = pose_to_dict(pose)\n",
    "    \n",
    "    return pose_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pose(frame, pose, frame_size_multiplier):\n",
    "    for i, joint_indices in enumerate(all_joints):\n",
    "            for joint_index in joint_indices:\n",
    "                (x, y, prob) = pose[joint_index]\n",
    "                if prob <= 0.6:\n",
    "                    continue\n",
    "                x,y = int(x*frame_size_multiplier), int(y*frame_size_multiplier)\n",
    "                joint_name = all_joints_names[i]\n",
    "                #print(x, y, all_joints_names[i])\n",
    "                cv2.circle(frame, (x, y), int(1.5*frame_size_multiplier), (255,0,0), 1)\n",
    "                cv2.putText(frame, \"{} - ({},{})\".format(joint_name, x, y), (x + int(1.3*frame_size_multiplier), y + int(1.3*frame_size_multiplier)), cv2.FONT_HERSHEY_DUPLEX, 0.2*frame_size_multiplier, (255, 255, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_exercise_detects(frame, lr_parts):\n",
    "    detectors = [\n",
    "        {'fn': detect_exercise1_pos1, 'results': [\n",
    "            {'expected': {'left': True}, 'message': 'Right Resting position', 'position': LEFT_RESTING_POSITION},\n",
    "             {'expected': {'right': True}, 'message': 'Left Resting position', 'position': RIGHT_RESTING_POSITION}\n",
    "        ]\n",
    "        },\n",
    "        {'fn': detect_exercise1_pos2, 'results': [\n",
    "           {'expected': {'left': True}, 'message': 'Right side arms position', 'position': LEFT_SIDE_POSITION},\n",
    "           {'expected': {'right': True}, 'message': 'Left side arms position', 'position': RIGHT_SIDE_POSITION} \n",
    "        ]}\n",
    "    ]\n",
    "    i_matched = 0\n",
    "    curr_positions = []\n",
    "    for detector in detectors:\n",
    "        fn = detector['fn']\n",
    "        res = fn(lr_parts)\n",
    "        expected_results = detector['results']\n",
    "        for expected_spec in expected_results:\n",
    "            kv = expected_spec['expected']\n",
    "            msg = expected_spec['message']\n",
    "            if all(k in res and res[k] == v for k,v in kv.items()):\n",
    "                i_matched+=1\n",
    "                curr_positions.append(expected_spec['position'])\n",
    "                cv2.putText(frame, \"{}\".format(msg), (30,i_matched*30), cv2.FONT_HERSHEY_DUPLEX, 1.2, (0, 0, 255))\n",
    "    return curr_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PhysiotherapyRoutine():\n",
    "    def __init__(self, profile, redis_cli):\n",
    "        self._profile = profile\n",
    "        self.routine_spec = profile['routine_spec']\n",
    "        self._set_number = 0\n",
    "        self._set_step_position = 0\n",
    "        self._remaining_steps = sum(len(r['steps']) for r in self.routine_spec) - 1\n",
    "        self._frame_text = None\n",
    "        self._is_completed = False\n",
    "        self._last_activity_detected_time = datetime.datetime.now()\n",
    "        self._redis_cli = redis_cli\n",
    "        \n",
    "    def start(self, intro_message):\n",
    "        # Speak first discription\n",
    "        print(\"Starting routine\")\n",
    "        self.speak(intro_message)\n",
    "        sleep(0.1)\n",
    "#         self.speak('Starting routine')\n",
    "        self._personalize_speak(self.routine_spec[0]['commentary'])\n",
    "        self._plot_text(self._remaining_steps, frame)\n",
    "    \n",
    "    def on_tick(self, frame):\n",
    "        self._plot_text(self._frame_text, frame)\n",
    "    \n",
    "    def _personalize_speak(self, msg):\n",
    "        try:\n",
    "            self.speak(\"{}, {}\".format(self._profile['name'], msg))\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    def speak(self, msg):\n",
    "        try:\n",
    "            self._redis_cli.rpush('speak', msg)\n",
    "        except:\n",
    "            pass\n",
    "        print(\"Speaking {}\".format(msg))\n",
    "    \n",
    "    def handle_detected_positions(self, detected_positions, frame):\n",
    "        expected_position_to_advance = self.routine_spec[self._set_number]['steps'][self._set_step_position]\n",
    "        if any([d == expected_position_to_advance for d in detected_positions]):\n",
    "            if self._is_detection_valid(detected_positions):\n",
    "                self._advance_to_next(frame)\n",
    "        \n",
    "    def _is_detection_valid(self, detected_positions):\n",
    "        # Check previous timstamp\n",
    "        if self._is_completed:\n",
    "            return False\n",
    "        detected_time = datetime.datetime.now()\n",
    "        if (detected_time - self._last_activity_detected_time).microseconds/1000 > 500:\n",
    "            self._last_activity_detected_time = detected_time\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _advance_to_next(self, frame):\n",
    "        if self._has_next_step_in_set():\n",
    "            self._set_step_position += 1\n",
    "            self._remaining_steps -= 1\n",
    "            self._plot_text(\"{}\".format(self.get_remaining_steps()), frame)\n",
    "        elif self._has_next_set():\n",
    "            self._set_number += 1\n",
    "            self._remaining_steps -= 1\n",
    "            self._set_step_position = 0\n",
    "            self._plot_text(\"{}\".format(self.get_remaining_steps()), frame)\n",
    "            # Commentary\n",
    "            self._speak_commentary()\n",
    "        else:\n",
    "            # Nothing left\n",
    "            self._is_completed = True\n",
    "            self._personalize_speak('This concludes your session for today, see you tomorrow!')\n",
    "    \n",
    "    def _speak_commentary(self):\n",
    "        self._personalize_speak(self.routine_spec[self._set_number]['commentary'])\n",
    "    \n",
    "    def _has_next_step_in_set(self):\n",
    "        try:\n",
    "            return self.routine_spec[self._set_number]['steps'][self._set_step_position+1] is not None\n",
    "        except:\n",
    "            return False\n",
    "    def _has_next_set(self):\n",
    "        try:\n",
    "            return self.routine_spec[self._set_number+1]['steps'][0] is not None\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    def _plot_text(self, text, frame):\n",
    "        self._frame_text = str(text)\n",
    "        cv2.putText(frame, str(text), (70,70), cv2.FONT_HERSHEY_DUPLEX, 1, (255, 255, 255))\n",
    "\n",
    "    def get_remaining_steps(self):\n",
    "        return self._remaining_steps\n",
    "#     def _has_next_step(self):\n",
    "#         return self._has_next_step_in_set() or self._has_next_set()\n",
    "    def is_completed(self):\n",
    "        return self._is_completed\n",
    "    \n",
    "    def is_running(self):\n",
    "        return not self._is_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_profile_in_frame(frame):\n",
    "    try:\n",
    "        face_locations = face_recognition.face_locations(frame)\n",
    "        face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "        for face_encoding in face_encodings:\n",
    "                # See if the face is a match for the known face(s)\n",
    "                #for name, profile in ENRICHED_PROFILES.items():\n",
    "                profile = list(ENRICHED_PROFILES.values());\n",
    "                encoding1 = profile[0]['face_encoding']\n",
    "                match = face_recognition.compare_faces(encoding1, face_encoding)\n",
    "                print(match)\n",
    "                if match[0]:\n",
    "                    return profile[0]\n",
    "                else:\n",
    "                    return None\n",
    "    except Exception as e:\n",
    "        # Silently eat the exception\n",
    "        raise e\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5bffad58c90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msmall_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mroutine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mroutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mfound_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_profile_in_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfound_profile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mroutine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhysiotherapyRoutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_profile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredis_cli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f6644020b186>\u001b[0m in \u001b[0;36mfind_profile_in_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Silently eat the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f6644020b186>\u001b[0m in \u001b[0;36mfind_profile_in_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;31m#for name, profile in ENRICHED_PROFILES.items():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENRICHED_PROFILES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mencoding1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'face_encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "resize_ratio = 0.55 # To be adjusted on basis of input frame size and resolution\n",
    "frame_size_multiplier = 1./resize_ratio\n",
    "\n",
    "process_this_frame = True\n",
    "frameNo = 1\n",
    "prev_pose = None\n",
    "center_x = None\n",
    "redis_cli = redis.Redis('localhost')\n",
    "routine = None\n",
    "found_profile = None\n",
    "completed_time = None\n",
    "is_in_completion_period = False\n",
    "\n",
    "while True:\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    ret, frame = video_capture.read(0)\n",
    "# for frame in read_from_stream('stream url'): ## If reading input from a different source and streaming via http server\n",
    "    frameNo += 1\n",
    "#     frame = cv2.flip( frame, -1 ) ## For inverting the frames\n",
    "    if completed_time is not None:\n",
    "        is_in_completion_period = time.time() - completed_time < 5\n",
    "        if is_in_completion_period:\n",
    "            continue\n",
    "    \n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=resize_ratio, fy=resize_ratio)\n",
    "    if routine is None or not routine.is_running():\n",
    "        found_profile = find_profile_in_frame(small_frame)\n",
    "        if found_profile:\n",
    "            routine = PhysiotherapyRoutine(found_profile, redis_cli)\n",
    "            # Start the session!\n",
    "            routine.start(intro_message=\"Hi {}, it is great to see you after yesterdays session.\".format(found_profile['name']))\n",
    "\n",
    "    if routine is not None and routine.is_running() and frameNo%7 == 0:\n",
    "        pose = detect_pose(small_frame)\n",
    "        pose_dict = filter_confident_pose_parts(pose_to_dict(pose))\n",
    "        prev_pose = pose\n",
    "        try:\n",
    "            curr_center = find_center_x(pose_dict)\n",
    "\n",
    "            if center_x is not None:\n",
    "                center_x = center_x*CENTER_CHANGE_ALPHA + (1-CENTER_CHANGE_ALPHA)*curr_center\n",
    "            else:\n",
    "                center_x = curr_center\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    if routine is not None and routine.is_running() and prev_pose is not None:\n",
    "        try:\n",
    "            plot_pose(frame, prev_pose, frame_size_multiplier)\n",
    "            lr_parts = get_left_right_parts(pose_dict, center_x)\n",
    "            detected_positions = plot_exercise_detects(frame, lr_parts)\n",
    "            routine.handle_detected_positions(detected_positions, frame)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    if routine is not None and routine.is_running():\n",
    "        routine.on_tick(frame)\n",
    "    if routine is not None and routine.is_completed():\n",
    "        routine = None\n",
    "        is_in_completion_period = True\n",
    "        completed_time = time.time()\n",
    "        found_profile = None\n",
    "    cv2.imshow('Video', frame) \n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "#cv2.destroyAllWindows()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_capture.release()\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
